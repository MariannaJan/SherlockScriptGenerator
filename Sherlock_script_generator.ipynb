{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sherlock script generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariannaJan/SherlockScriptGenerator/blob/master/Sherlock_script_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kd6PZZxCKV6",
        "colab_type": "text"
      },
      "source": [
        "# BBC Sherlock script generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQaCZneSPVMP",
        "colab_type": "text"
      },
      "source": [
        "Natural language processing (NLP) offers us many ways to use text - from getting certain proper names (Named Entity Recognition - NER), through classifing the text according to emotions it confers (sentiment analysis) to automatic creation of summaries and automatic translation.\n",
        "\n",
        "NLP offers us also a possibility to generate new, unique texts, on the basis of a chosen dataset. Generally, the bigger the dataset, the better the results. This creative aspect of NLP is used in chatbots, especially with other language processing methods.\n",
        "\n",
        "There are many ways to approach text generation with machine learning. What is shown here, is a simple Recurrent Neural Network based on Long Short Term Memory Cells (LSTM), that does not use any language model, but instead is based on the assumption, that you can predict a probable next word if you know the words that precede it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9quoz09KOuf3",
        "colab_type": "text"
      },
      "source": [
        "![Sherlock and Watson](https://media.giphy.com/media/3osxYAEY9vBn8BkS0U/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab9s8IzICgnE",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfUXxTgHDDuh",
        "colab_type": "text"
      },
      "source": [
        "First we need to get the txt file with the series transcript.\n",
        "\n",
        "The transcripts are gathered from [here](https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=sherlock)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMvESc1tCZ3q",
        "colab_type": "code",
        "outputId": "a71544d7-d215-47a1-83de-868d7f840e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "!wget -O sherlock.txt https://www.dropbox.com/s/od4a3cowfu3sezm/Sherlock_script.txt?dl=0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-08 10:07:34--  https://www.dropbox.com/s/od4a3cowfu3sezm/Sherlock_script.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/od4a3cowfu3sezm/Sherlock_script.txt [following]\n",
            "--2019-06-08 10:07:35--  https://www.dropbox.com/s/raw/od4a3cowfu3sezm/Sherlock_script.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc849916a3cf4330377ee4766b9f.dl.dropboxusercontent.com/cd/0/inline/Aiax3UXjf-qTh4HfhAqIRhyN-O5P_90JsCcJe46hGs1QDxL9zX_wqxvEWi3MhFRUhddmmaZkV3qHaA0PbT7urMpHp5ACGnEe64C18FNqWxzSfA/file# [following]\n",
            "--2019-06-08 10:07:35--  https://uc849916a3cf4330377ee4766b9f.dl.dropboxusercontent.com/cd/0/inline/Aiax3UXjf-qTh4HfhAqIRhyN-O5P_90JsCcJe46hGs1QDxL9zX_wqxvEWi3MhFRUhddmmaZkV3qHaA0PbT7urMpHp5ACGnEe64C18FNqWxzSfA/file\n",
            "Resolving uc849916a3cf4330377ee4766b9f.dl.dropboxusercontent.com (uc849916a3cf4330377ee4766b9f.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uc849916a3cf4330377ee4766b9f.dl.dropboxusercontent.com (uc849916a3cf4330377ee4766b9f.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 741017 (724K) [text/plain]\n",
            "Saving to: ‘sherlock.txt’\n",
            "\n",
            "sherlock.txt        100%[===================>] 723.65K  2.62MB/s    in 0.3s    \n",
            "\n",
            "2019-06-08 10:07:36 (2.62 MB/s) - ‘sherlock.txt’ saved [741017/741017]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqvFASdLZUcH",
        "colab_type": "text"
      },
      "source": [
        "Then we read the text file and save it as string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuKWyV3eC-w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sherlock.txt', 'r') as f:\n",
        "  transcript = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt0cM6WIcH_r",
        "colab_type": "text"
      },
      "source": [
        "## Setup for saving onto Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi5i4FKwcQvt",
        "colab_type": "text"
      },
      "source": [
        "We need to mout the drive and specify the write / read location on it. We also import datetime module for future use in naming the files for saving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxtX8YubcHuq",
        "colab_type": "code",
        "outputId": "d750577d-669a-4fbd-b0fa-cc2f26d46733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#mounting google drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB8wEs1Rc51F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "project_dir = \"/content/gdrive/My Drive/Script_gen/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NVD4fANZxw5",
        "colab_type": "text"
      },
      "source": [
        "##Exploring the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJLZK0CsZgxA",
        "colab_type": "text"
      },
      "source": [
        "Next we explore the data to know what preprocessing steps it would need for the model to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bg5hKzYF-MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = transcript.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlcrsJYObpCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line_lengths = [len(line.split()) for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT8d4F4Fa4da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getUniqueWords(text):\n",
        "  words = text.lower().split()\n",
        "  return list(set(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJHiAlZedUYB",
        "colab_type": "text"
      },
      "source": [
        "Basic stats for the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HEyMtbfaK2j",
        "colab_type": "code",
        "outputId": "896454f1-5c92-4fd5-adf5-8b6a76fe6121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('number of lines: ', len(lines))\n",
        "print('average number of words in a sentence: ', round(np.average(line_lengths), 1))\n",
        "print('number of words: ', len(transcript.lower().split()))\n",
        "print('number of unique words: ', len(getUniqueWords(transcript)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of lines:  15583\n",
            "average number of words in a sentence:  8.7\n",
            "number of words:  135946\n",
            "number of unique words:  15641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWvfeLb4PP0a",
        "colab_type": "text"
      },
      "source": [
        "For reference english language has 171476 words (based on Oxford English dictionary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahRw_vAjhKBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getUniqueChars(text):\n",
        "  uniqueChars = list(set([char for char in text.lower()]))\n",
        "  uniqueChars.sort()\n",
        "  return uniqueChars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nTRWLkAGCvx",
        "colab_type": "code",
        "outputId": "94ba48f0-15cf-46cb-98c7-bf23fabb1a98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#uniqueChars = list(set([char for char in transcript.lower()]))\n",
        "#uniqueChars.sort()\n",
        "\n",
        "\n",
        "print('There are {} unique characters in the dataset, counting lower and upper case letters as one letter'.format(len(getUniqueChars(transcript))))\n",
        "print(getUniqueChars(transcript))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 67 unique characters in the dataset, counting lower and upper case letters as one letter\n",
            "['\\n', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '\\x80', '\\x8d', '\\x99', '£', '©', '°', '½', 'â', 'ã', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X13mtCcIeuGt",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylI4_7jqezLG",
        "colab_type": "text"
      },
      "source": [
        "As we have seen there is a number of characters in the dataset, that we could safely ommit, without loosing the meaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmoghA8kdI0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "def cleanupText(text):\n",
        "  wanted_chars = string.digits + string.ascii_letters + \"'\" + '\".,:;!?-() \\n'\n",
        "  print(wanted_chars)\n",
        "  return ''.join(x for x in text if x in (wanted_chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBwbtj01QdxU",
        "colab_type": "code",
        "outputId": "8d4a819b-0645-4ea5-b02a-128238d89f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "transcript = cleanupText(transcript)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\".,:;!?-() \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUc4nSLNPadB",
        "colab_type": "code",
        "outputId": "4c660d70-443f-4c51-9b6a-eab3b00da44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('After cleanup there are {} unique characters in the dataset'.format(len(getUniqueChars(transcript))))\n",
        "print(getUniqueChars(transcript))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After cleanup there are 49 unique characters in the dataset\n",
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t-yNK0DSnEF",
        "colab_type": "text"
      },
      "source": [
        "One more step we need to take is to tokenize symbols (punctuation), as without it, the dataset after splitting to single words would have several varaiants of the same word with different punctuation, eg. 'go', 'go?', 'go.',  'go! and so on. To that end we can replace those symbols with spoecial tokens and delimit them with space, so that they will be treated as separate words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt6eMiIpThlI",
        "colab_type": "text"
      },
      "source": [
        "First we need to create a dictionary for the symbol tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeSKLm_4ipvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punctuation_dict = {\n",
        "        '.':'||period||',\n",
        "        ',':'||comma||',\n",
        "        '\"':'||quote||',\n",
        "        ';':'||semicolon||',\n",
        "        '!':'||exclamation||',\n",
        "        '?':'||question||',\n",
        "        '(':'||leftPar||',\n",
        "        ')':'||rightPar||',\n",
        "        '-':'||dash||',\n",
        "        '\\n':'||return||',\n",
        "        \"'\":'||single_quote||'\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9mGLtJGVP61",
        "colab_type": "text"
      },
      "source": [
        "Then we change all the punctuation in the dataset with the appropriate tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5irDtVVKcUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizePunctuation(text):\n",
        "  for key, value in punctuation_dict.items():\n",
        "    text = text.replace(key, ' {} '.format(value))\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjTLpTxZXOVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transcript = tokenizePunctuation(transcript)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91nf6iCMXYuC",
        "colab_type": "code",
        "outputId": "78db34c3-2a38-493d-85b4-3d52ee6845a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(transcript[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ||dash|| ELLA: How ||single_quote|| s your blog going ||question||   ||dash|| Hmm ||comma||  fine |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4ng_gINYWEO",
        "colab_type": "text"
      },
      "source": [
        "Now we can tokenize the dataset, which in our case will be splitting it into single words. This will allow us to create a lookup table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Uq0pAeXbxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words = getUniqueWords(transcript)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCPoKquzJzOy",
        "colab_type": "code",
        "outputId": "04d3a817-8de7-4d54-93fb-caf5ccce6ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('The number of unique words is: ', len(unique_words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of unique words is:  8506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtLKnLC3Lb48",
        "colab_type": "text"
      },
      "source": [
        "As we can see the tokenization of punctuation lowered the number of unique words almost by half."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ONc5RskRwrC",
        "colab_type": "text"
      },
      "source": [
        "Now we need to create the lookup table, to convert the words in the dataset into nubers. We will do this by creating a dictionary of words as keys, and integers as values. We will do this by first sorting the words according to how often they appear in the dataset, so that the more often the word appears, the lower integer it will get..\n",
        "\n",
        "We will also need a lookup table allowing us to convert numbers generated by our model back into words, so we'll create another dictionary for that, which will be the first dictionary with keys and values swapped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij7zmvA2J-F6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def createLookupTables(text, padding_symbol):\n",
        "  text = text.lower().split()\n",
        "  text.append(padding_symbol)\n",
        "  count = Counter(text)\n",
        "  vocabulary = sorted(count, key=count.get, reverse=True)\n",
        "  word_to_int = {word:idx for idx, word in enumerate(vocabulary)}\n",
        "  int_to_word = {idx:word for idx, word in enumerate(vocabulary)}\n",
        "  return (word_to_int,int_to_word)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2NAomVyVjVe",
        "colab_type": "text"
      },
      "source": [
        "To allow for generation of sequences (that is parts of a new script), we'll need a padding symbol, so we'll add it to our lookup tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcIfi1iiVyCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PADDING_SYMBOL = '<PAD>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krtgzilJVHSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_int,int_to_word = createLookupTables(transcript, PADDING_SYMBOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec0xJ0T9VQSS",
        "colab_type": "code",
        "outputId": "f5f2b6b1-7c9b-49e0-eff5-24ad42840cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for i in range(5):\n",
        "  print(int_to_word[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "||return||\n",
            "||period||\n",
            "||comma||\n",
            "||single_quote||\n",
            "you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDFKpQmEkP4m",
        "colab_type": "text"
      },
      "source": [
        "As the neural networks are in essence a series of linear equations, they work on numbers and not on words. We'll use out word_to_int dictionary the dataset into a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h34xdMgWkpLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def changeIntoInts(text):\n",
        "  return [word_to_int[word] for word in text.lower().split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh9tY8t9lHQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transcript_int = changeIntoInts(transcript)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe6dKknXnlFV",
        "colab_type": "text"
      },
      "source": [
        "This is how the beginning of the transcript looks after all the changes we have made:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xlueM_SlRFU",
        "colab_type": "code",
        "outputId": "adbd3954-73a6-4234-8d71-51e6e896ccd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(transcript_int[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8, 4733, 55, 3, 10, 29, 388, 73, 5, 8, 170, 2, 215, 1, 0, 93, 1, 0, 98, 93]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5C6uOVrftDh",
        "colab_type": "text"
      },
      "source": [
        "### Saving and loading the results of preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTbnb659f78Z",
        "colab_type": "text"
      },
      "source": [
        "We can now save our lookup tables, punctuation dictionary and prepared transcript (in the form of a list of integers) for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0b7RQ5ejTDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ-o2ZBGgJ4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_preprocessed(word_to_int, int_to_word, punctuation_dict, transcript_int, path):  \n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump((word_to_int, int_to_word, punctuation_dict, transcript_int), f)\n",
        "    print('Saved preprocessed data in', path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2PHppiJhXaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_preprocessed(path):\n",
        "  return pickle.load(open(path, mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvHFBxHTiHkz",
        "colab_type": "code",
        "outputId": "a242b982-3bba-469d-f815-89a40fc4f415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#saving current preprocessed data\n",
        "path = project_dir + 'preprocessed' + str(datetime.datetime.now()) +'.p'\n",
        "save_preprocessed(word_to_int, int_to_word, punctuation_dict, transcript_int, path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved preprocessed data in /content/gdrive/My Drive/Script_gen/preprocessed2019-05-12 13:25:34.233846.p\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLMmOCxAWGv0",
        "colab_type": "text"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLHp_QbhfLyx",
        "colab_type": "text"
      },
      "source": [
        "### Model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCzqSxfQfEPw",
        "colab_type": "text"
      },
      "source": [
        "To build the script generatorl we'll build a RNN ( [Recurrent Neural Network](https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce)) based on LSTM (Long-Short Term Memory) cells. \n",
        "\n",
        "RNNs are a specific kind of neural networks, that differ from vanilla neural networks by taking as input a series of data (like a time series) instead of a fixed size vector. This allows for concideration of relations of input itms in time. This means, that the result of analysing the input is influenced also by preciding inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eri87eoQIHx",
        "colab_type": "text"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcZlGrObmIJc",
        "colab_type": "text"
      },
      "source": [
        "To create a dataset that is adequate for this job, we'll turn it into batches, with features being a specified number of words preceding the word that is concidered our target (or label).\n",
        "\n",
        "To provide a consistent format for the dataset we'll use [TensorDataset](https://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) and to turn it into appropriate batches we'll use [DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader).\n",
        "\n",
        "The TensorDataset needs PyTorch Tensors as arguments. To turn a simple Pyton list into a Tensor, we'll use [torch.from_numpy](https://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy) , and that's why we need to convert the list into a numpy array first.\n",
        "\n",
        "To allow for flexible adjustment of the dataloader for the model, in the fiunction creating it, we'll concider two additional parameters:\n",
        "\n",
        "\n",
        "*   sequence length - this specifies how many words before the target word are concidered to be features\n",
        "*   batch size - this specifies, how many feature / target sets are in a batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGqpjCoZVxr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def createDataloader(text_int, sequence_length, batch_size):\n",
        "  \n",
        "  #first we need to convert the whole dataset into features and targets lists\n",
        "  features, target = [], []\n",
        "  \n",
        "  #we loop through the whole dataset, moving one element at a time\n",
        "  #we start at the first element and end at the element that is sequence_length from the end of the dataset\n",
        "  \n",
        "  for sequence_no in range(len(text_int)-sequence_length):\n",
        "    features.append(text_int[sequence_no:sequence_no+sequence_length])\n",
        "    target.append(text_int[sequence_no+sequence_length])\n",
        "    \n",
        "  #then we convert the dataset into the Tensor Dataset, which accepts NumPy arrays as parameter\n",
        "  #Tensor Dataset needs features and target nparrays as parameters\n",
        "  \n",
        "  dataset = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array(target)))\n",
        "\n",
        "  #once we have the appropriate dataset, we can create the dataloder\n",
        "  \n",
        "  dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
        "  \n",
        "  return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTh5Bt6xu0Q",
        "colab_type": "text"
      },
      "source": [
        "The dataloader is an iterator, that returns fetures and target in the form of tensors, that form a batch. For a first batch in the dataset, and chosen parameters, the batch looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PNbqvRoxX0_",
        "colab_type": "code",
        "outputId": "4fd0268a-b2d9-4d86-cfb7-cec0212cc0bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "sample_loader = createDataloader(transcript_int[:50], sequence_length=5, batch_size=10)\n",
        "sample_iterable = iter(sample_loader)\n",
        "sample_feature, sample_target = sample_iterable.next()\n",
        "\n",
        "print('Sample feature batch:')\n",
        "print(sample_feature)\n",
        "print('Sample target:')\n",
        "print(sample_target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample feature batch:\n",
            "tensor([[  93,    1,    0,   98,   93],\n",
            "        [   9,    3,   10,   73,   12],\n",
            "        [   1,    0,   65,    2,    9],\n",
            "        [  73,    5,    8,  170,    2],\n",
            "        [  27,   11,  288,    1,    0],\n",
            "        [ 425,   12, 3332,   12, 2198],\n",
            "        [   0,   98,   93,    1,    0],\n",
            "        [ 288,    1,    0,   65,    2],\n",
            "        [   8,   27,   11,  288,    1],\n",
            "        [  10,   73,   12,  140,    4]])\n",
            "Sample taget:\n",
            "tensor([  1, 140,   3, 215,  65, 184,   8,   9,   0,  11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLruI2LTQMv-",
        "colab_type": "text"
      },
      "source": [
        "### Defining the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxn8OqiuQzCQ",
        "colab_type": "text"
      },
      "source": [
        "First we need to define the model - we'll use Recurrent Neural Network. To impement this, we'll use PyTorch [nn.Module](https://pytorch.org/docs/master/nn.html#torch.nn.Module), that allows us to construct neural network models.\n",
        "\n",
        "The init function specifies the structure of the neural network. It accepts several parameters, to allow for tutning the model:\n",
        "\n",
        "*   vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
        "*   output_size: The number of output dimensions of the neural network\n",
        "*  embedding_dim: The size of embeddings\n",
        "*  hidden_dim: The size of the hidden layer outputs\n",
        "*  dropout: dropout to add in between LSTM layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsxAxOEsMic4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ScriptGenModel(nn.Module):\n",
        "  \n",
        "  #here we define the structure of the model\n",
        "  \n",
        "  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
        "    super(ScriptGenModel, self).__init__()\n",
        "    \n",
        "    #saving the parameters for future use\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "        \n",
        "    # defining model layers\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "  #here we decide, how the forward pass of the model should be performed\n",
        "  \n",
        "  def forward(self, nn_input, hidden):\n",
        "    \n",
        "    batch_size = nn_input.size(0)\n",
        "\n",
        "    embeds = self.embedding(nn_input)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "    out = self.fc(lstm_out)\n",
        "\n",
        "    out = out.view(batch_size, -1, self.output_size)\n",
        "    out = out[:, -1]\n",
        "    return out, hidden\n",
        "  \n",
        "  #here we define the way to initialise the hidden state\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FCvFBjRWK7w",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkvX-07oYyat",
        "colab_type": "text"
      },
      "source": [
        "### Training functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEOs8h8BWOcv",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the model defined, we need to prepare the training functions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59YSQkScVet_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_back_prop(scriptGenModel, optimizer, criterion, inp, target, hidden):\n",
        "  # move data to GPU, if available\n",
        "  if(train_on_gpu):\n",
        "    inp, target = inp.cuda(), target.cuda()\n",
        "    \n",
        "  # perform backpropagation and optimization\n",
        "  hidden = tuple([each.data for each in hidden])\n",
        "\n",
        "  scriptGenModel.zero_grad()\n",
        "    \n",
        "  output, hidden = scriptGenModel(inp, hidden)\n",
        "\n",
        "  loss = criterion(output, target)\n",
        "  loss.backward()\n",
        "  nn.utils.clip_grad_norm_(scriptGenModel.parameters(), 3)\n",
        "  optimizer.step()\n",
        "  return loss.item(), hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx6Omd9-YEMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_scriptGenModel(scriptGenModel, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
        "    batch_losses = []\n",
        "    \n",
        "    scriptGenModel.train()\n",
        "\n",
        "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
        "    for epoch_i in range(1, n_epochs + 1):\n",
        "        \n",
        "        # initialize hidden state\n",
        "        hidden = scriptGenModel.init_hidden(batch_size)\n",
        "        \n",
        "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "            \n",
        "            # make sure you iterate over completely full batches, only\n",
        "            n_batches = len(train_loader.dataset)//batch_size\n",
        "            if(batch_i > n_batches):\n",
        "                break\n",
        "            \n",
        "            # forward, back prop\n",
        "            loss, hidden = forward_back_prop(scriptGenModel, optimizer, criterion, inputs, labels, hidden)          \n",
        "            # record loss\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "            # printing loss stats\n",
        "            if batch_i % show_every_n_batches == 0:\n",
        "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
        "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
        "                batch_losses = []\n",
        "\n",
        "    # returns a trained rnn\n",
        "    return scriptGenModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loRb0X1BY3i2",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aetRHMoPY6sQ",
        "colab_type": "text"
      },
      "source": [
        "For the training of the model to work properly, we need to decide on the hypeparameters we want to use for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud9gV2mVY5xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters for data preprocessing\n",
        "\n",
        "sequence_length = 8\n",
        "batch_size = 64\n",
        "\n",
        "# Training parameters\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model parameters\n",
        "\n",
        "vocab_size = len(word_to_int)\n",
        "output_size = vocab_size\n",
        "embedding_dim = 300\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "# Show stats for every n number of batches\n",
        "\n",
        "show_every_n_batches = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k42-vakCb_fr",
        "colab_type": "text"
      },
      "source": [
        "### Preparing for saving / loading model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1d4ZAwVcKlR",
        "colab_type": "text"
      },
      "source": [
        "As we train the model we will save it on Google Drive, so that we can use the checkpoint that satisfies us best, if we decide to tune the hyperparameters. We will also define the method for loading a saved checkpoint for future use, like generating new scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHUwQyIQcwnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(filename, scriptGenModel):\n",
        "  model_save_name = filename + \".pt\"\n",
        "  path = project_dir + model_save_name\n",
        "  torch.save(scriptGenModel, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SwncCIceWIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(filename):\n",
        "    path = project_dir + filename + \".pt\"\n",
        "    return torch.load(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdq_nC-EfgPc",
        "colab_type": "text"
      },
      "source": [
        "### Running the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LqPutK8f407",
        "colab_type": "text"
      },
      "source": [
        "We can finally start the training of our model. First we'll check if training on GPU is available, as training this kind of model on CPU would be really time consuming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVaRAwrjf3Xa",
        "colab_type": "code",
        "outputId": "42138c68-0268-4910-d22d-3df396bce47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#checking if GPU is available\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "  print('There is no GPU available. Please consider switching to GPU for training the model.')\n",
        "else:\n",
        "  print(\"GPU available. You're good to go!\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available. You're good to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kFZ_K5PgyFV",
        "colab_type": "text"
      },
      "source": [
        "Next we need to create the dataloder for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEFRotxufRpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = createDataloader(transcript_int, sequence_length, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpGPUXJkiBLJ",
        "colab_type": "text"
      },
      "source": [
        "We instantiate the model. Before training the model we have to decide on the optimiser and loss function we'll use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2UEC4b1hjAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creaing the model\n",
        "scriptGenModel = ScriptGenModel(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
        "\n",
        "#deciding on optimizer and loss functions\n",
        "optimizer = torch.optim.Adam(scriptGenModel.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH1OSOhijc1Z",
        "colab_type": "text"
      },
      "source": [
        "We can see how our model looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzJ8hpmriwRI",
        "colab_type": "code",
        "outputId": "92c9aaac-a4d2-448f-b128-1ad62a83f745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(scriptGenModel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ScriptGenModel(\n",
            "  (embedding): Embedding(8507, 300)\n",
            "  (lstm): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=8507, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5adNqJxpjrtj",
        "colab_type": "text"
      },
      "source": [
        "We can finnaly start the training of the model. When the training is finished, the model will be saved to Google Drive. We'll add the current time to the name of the saved model, to make sure, that we don't overwrite a previous model by accident."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He2dcG05jZcg",
        "colab_type": "code",
        "outputId": "516cbd65-144b-469d-bf7b-bfd9574e7889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2128
        }
      },
      "source": [
        "#moving the model to GPU if it's available\n",
        "if train_on_gpu:\n",
        "    scriptGenModel.cuda()\n",
        "\n",
        "#running the trainining loop\n",
        "trained_scriptGenModel = train_scriptGenModel(scriptGenModel, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
        "\n",
        "#Setup for saving the trained model to Google Drive\n",
        "current_time = datetime.datetime.now() \n",
        "model_name = 'trained_scriptGenModel' + str(current_time)\n",
        "save_model(model_name, trained_scriptGenModel)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 10 epoch(s)...\n",
            "Epoch:    1/10    Loss: 5.223743494510651\n",
            "\n",
            "Epoch:    1/10    Loss: 4.690614940643311\n",
            "\n",
            "Epoch:    1/10    Loss: 4.528136008262634\n",
            "\n",
            "Epoch:    1/10    Loss: 4.424260503292084\n",
            "\n",
            "Epoch:    1/10    Loss: 4.378831959247589\n",
            "\n",
            "Epoch:    1/10    Loss: 4.304594363689422\n",
            "\n",
            "Epoch:    2/10    Loss: 4.091347557698895\n",
            "\n",
            "Epoch:    2/10    Loss: 4.055635792732239\n",
            "\n",
            "Epoch:    2/10    Loss: 4.004753903865814\n",
            "\n",
            "Epoch:    2/10    Loss: 3.975473068714142\n",
            "\n",
            "Epoch:    2/10    Loss: 3.968696400642395\n",
            "\n",
            "Epoch:    2/10    Loss: 3.947124272823334\n",
            "\n",
            "Epoch:    3/10    Loss: 3.8058034220855395\n",
            "\n",
            "Epoch:    3/10    Loss: 3.71501554107666\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7199957451820373\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7051987519264222\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7419188141822817\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6934643173217774\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5206771789196725\n",
            "\n",
            "Epoch:    4/10    Loss: 3.4885264630317687\n",
            "\n",
            "Epoch:    4/10    Loss: 3.494842125892639\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5142241282463074\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5108580594062806\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5035486097335817\n",
            "\n",
            "Epoch:    5/10    Loss: 3.2762646089770837\n",
            "\n",
            "Epoch:    5/10    Loss: 3.2586976051330567\n",
            "\n",
            "Epoch:    5/10    Loss: 3.2725155472755434\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3300962285995483\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3167977809906004\n",
            "\n",
            "Epoch:    5/10    Loss: 3.362126236438751\n",
            "\n",
            "Epoch:    6/10    Loss: 3.1329444436256044\n",
            "\n",
            "Epoch:    6/10    Loss: 3.0817730422019958\n",
            "\n",
            "Epoch:    6/10    Loss: 3.1143279123306273\n",
            "\n",
            "Epoch:    6/10    Loss: 3.1387056622505187\n",
            "\n",
            "Epoch:    6/10    Loss: 3.1376552758216856\n",
            "\n",
            "Epoch:    6/10    Loss: 3.167438891887665\n",
            "\n",
            "Epoch:    7/10    Loss: 2.953521545061808\n",
            "\n",
            "Epoch:    7/10    Loss: 2.936293813467026\n",
            "\n",
            "Epoch:    7/10    Loss: 2.959650285720825\n",
            "\n",
            "Epoch:    7/10    Loss: 2.9951196341514588\n",
            "\n",
            "Epoch:    7/10    Loss: 3.0176130418777465\n",
            "\n",
            "Epoch:    7/10    Loss: 3.0455510020256042\n",
            "\n",
            "Epoch:    8/10    Loss: 2.8085466976651174\n",
            "\n",
            "Epoch:    8/10    Loss: 2.806438776254654\n",
            "\n",
            "Epoch:    8/10    Loss: 2.8440777769088745\n",
            "\n",
            "Epoch:    8/10    Loss: 2.874288646697998\n",
            "\n",
            "Epoch:    8/10    Loss: 2.882727725982666\n",
            "\n",
            "Epoch:    8/10    Loss: 2.92174959897995\n",
            "\n",
            "Epoch:    9/10    Loss: 2.6979845023440743\n",
            "\n",
            "Epoch:    9/10    Loss: 2.6933357026576994\n",
            "\n",
            "Epoch:    9/10    Loss: 2.703310260295868\n",
            "\n",
            "Epoch:    9/10    Loss: 2.7702316715717314\n",
            "\n",
            "Epoch:    9/10    Loss: 2.793613357067108\n",
            "\n",
            "Epoch:    9/10    Loss: 2.8058886280059814\n",
            "\n",
            "Epoch:   10/10    Loss: 2.610765489454041\n",
            "\n",
            "Epoch:   10/10    Loss: 2.6127942473888397\n",
            "\n",
            "Epoch:   10/10    Loss: 2.609738887786865\n",
            "\n",
            "Epoch:   10/10    Loss: 2.676770941019058\n",
            "\n",
            "Epoch:   10/10    Loss: 2.6957201845645904\n",
            "\n",
            "Epoch:   10/10    Loss: 2.7052315790653227\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ScriptGenModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bLW-8D8RL5k",
        "colab_type": "text"
      },
      "source": [
        "## Generating the script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jU3DX_8RVmq",
        "colab_type": "text"
      },
      "source": [
        "Now that we finally have our trained model, we can try and generate a new script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FnY6VrTRR_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(scriptGenModel, prime_id, int_to_vocab, punctuation_dict, pad_value, predict_len=100):\n",
        "\n",
        "    scriptGenModel.eval()\n",
        "    \n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "    \n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "            current_seq = torch.LongTensor(current_seq)\n",
        "        \n",
        "        # initialize the hidden state\n",
        "        hidden = scriptGenModel.init_hidden(current_seq.size(0))\n",
        "        \n",
        "        # get the output of the model\n",
        "        \n",
        "        ##### Added next 2 lines to fix numpy bug  ####\n",
        "        scriptGenModel.cpu()\n",
        "        current_seq = current_seq.cpu()\n",
        "        hidden = hidden[0].cpu(), hidden[1].cpu()\n",
        "        #print(\" curs:\", current_seq.device, \"hidden[0]\", hidden[0].device, \"hidden[1]\", hidden[1].device)\n",
        "        output, _ = scriptGenModel(current_seq, hidden)\n",
        "        \n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "         \n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "        \n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)     \n",
        "        \n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq, -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    \n",
        "    gen_sentences = ' '.join(predicted)\n",
        "    \n",
        "    # Replace punctuation tokens\n",
        "    for key, token in punctuation_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "    \n",
        "    # return all the sentences\n",
        "    return gen_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi_w_tAaS3Pp",
        "colab_type": "text"
      },
      "source": [
        "We setup basic values for the generator function and see the result.\n",
        "\n",
        "We load the trained model and saved parameters, if they are not defined straight after training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTIWoT-QSDnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we check if the parameters for text generation are loaded, and if not we load them in\n",
        "\n",
        "try:\n",
        "  if (word_to_int and int_to_word and punctuation_dict and PADDING_SYMBOL):\n",
        "    print('Data for text generation present')\n",
        "except:\n",
        "  path = project_dir+'preprocessed.p'\n",
        "  word_to_int, int_to_word, punctuation_dict, transcript_int = load_preprocessed(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75PjatYGWITe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we check if the model is present, and if not we load it in\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  if (trained_scriptGenModel):\n",
        "    print('The model for text generation is present')\n",
        "except:\n",
        "  trained_scriptGenModel = load_model('trained_scriptGenModel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36PmiAIOSeZb",
        "colab_type": "code",
        "outputId": "06ce05f0-d12f-4919-e7d4-88d672a0c013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "gen_length = 400 \n",
        "prime_word = 'I'\n",
        "\n",
        "generated_script = generate(trained_scriptGenModel, word_to_int[prime_word.lower()], int_to_word, punctuation_dict, word_to_int[PADDING_SYMBOL], gen_length)\n",
        "print(generated_script)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i' d say is your best friend.\n",
            "- oh, i see.\n",
            "- you' re a doctor, john.\n",
            "what? what? the hiker turns up with me!' i' m sorry.\n",
            "i' m afraid you acquainted me, i' m just wondering.\n",
            "oh, i think you' re going to need jones and abby the merchant' s drinking habits out.\n",
            "- i don' t know.\n",
            "- i am.\n",
            "- no.\n",
            "- no, no, i don' t think so.\n",
            "i' m just saying.\n",
            "you' ll need some shopping, because i' ll have to go to the good details.\n",
            "- i' ve seen it.\n",
            "no, it' s all.\n",
            "i know.\n",
            "you' re going to die.\n",
            "i know you' ve got a gun.\n",
            "oh- you think i' ll take the case?- i need to know.\n",
            "- what do you mean?- i didn' t know.\n",
            "- you were a doctor.\n",
            "- i' ll skip you in tomorrow.\n",
            "' you realise i' ve got bluebell.\n",
            "i don' t want the money.\n",
            "- what?- nothing, buddy hacker.\n",
            "- you spoke to her husband being executed.\n",
            "it' s a game of chess with a bullet through his head? it must be a bit mundane but i was able to help the target ones.\n",
            "i' m afraid you can.\n",
            "i just thought that' ll be fine.\n",
            "i' ll be late for this place.\n",
            "it' s a memory pet, but there' s nothing that links the most energy system of your treacherous.\n",
            "- you know what you did!- no! don' t worry, i' m just going to ring this thread doorbell rings i' m a private doctor.\n",
            "- what?! i need to know what he does.\n",
            "- what are you looking at?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8_h6Fxqnneh",
        "colab_type": "text"
      },
      "source": [
        "We can now save the generated script to a file if we like it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFbOA0v6XiUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saveGenScript(generated_script):\n",
        "  path = project_dir + 'generated_script' + str(datetime.datetime.now()) + '.txt'\n",
        "  with open(path, \"w\") as text_file:\n",
        "    text_file.write(generated_script)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoBtaYKyoeWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving generated script to Google Drive\n",
        "\n",
        "saveGenScript(generated_script)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwfYIF4gpVpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}